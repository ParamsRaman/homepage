---
layout: default
section: research 
title: "Parameswaran Raman: Research"
---

<style>
  li.paper {
    margin-left: 20px;
    margin-bottom: 10px;
  }

  li.paper .title {
    font-size: 115%;
  }

  li.paper .me {
    background-color: #eeffee;
  }
</style>

<!-- Not ready yet
<pre style="background-color: #eeeeff">
  <i class="icon-info-sign"></i> Click on the paper title for more information
</pre>
-->

<h2> Publications</h2>

<!--h2> Conference Papers </h2-->
<ul>
  <li class="paper">
    <span class="title">
      Ranking via Robust Binary Classification<br>
    </span>
    Hyokun Yun, <span class="me">Parameswaran Raman</span>, S.V.N Vishwanathan <br>
    <b>NIPS 2014 </b>
    <a href="http://papers.nips.cc/paper/5363-ranking-via-robust-binary-classification.pdf">[Paper]</a>
    <a href="static/pub/RoBiRank_Poster.pdf">[Poster]</a>
    <a href="https://bitbucket.org/d_ijk_stra/robirank">[Code]</a>
    <!--a href="robirank.html">[Code]</a--><br/>
    <p><small>Summary: We propose RoBiRank - a Learning to Rank algorithm inspired by Robust Binary Classification and show that it scales well on large-data. The main idea behind Robust Binary Classification is to use transformation on convex losses to help give up performance on hard to classify data points (outliers). <u>Firstly</u>, we observe that this is related to learning to rank, where we would not mind sacrificing accuracy at the bottom of the ranking list in order to gain performance at top of the list. We thus show that our ranking objective function can be viewed as a generalization of robust binary classification. <u>Secondly</u>, minimizing RoBiRank is equivalent to directly maximizing DCG (a popular evaluation metric for listwise learning to rank). As a result, RoBiRank performs really well at the top of the list. <u>Thirdly</u>, using a linearization trick on our loss allows us to obtain an unbiased stochastic gradient estimator so that our SGD optimizer becomes independent of the size of the dataset. In addition, our algorithm is parallelizable and can also be used to solve large-scale problems without any explicit features. Experimental results are shown on both medium and very large datasets.</small></p>
    <!-- 25 out of 215 (11.6%) -->
  </li>
  
  <li class="paper">
    <span class="title">
      DS-MLR: Exploiting Double Separability for Scaling up Distributed Multinomial Logistic Regression<br>
    </span>
    <span class="me">Parameswaran Raman</span>, Sriram Srinivasan, Shin Matsushima, Xinhua Zhang, Hyokun Yun, S.V.N Vishwanathan <br>
    <b>pre-print</b>
    <a href="http://arxiv.org/pdf/1604.04706v1.pdf">[arXiv]</a>
    <!--a href="robirank.html">[Code]</a--><br/>
    <p><small>Summary: Multinomial logistic regression is a popular tool in the arsenal of machine learning algorithms, yet scaling it to datasets with very large number of data points and classes has not been trivial. This is primarily because one needs to compute the log-partition function on every data point. This makes distributing the computation hard. In this paper, we present a distributed stochastic gradient descent based optimization method (DS-MLR) for scaling up multinomial logistic regression problems to very large data. Our algorithm exploits double-separability, an attractive property we observe in the objective functions of several models in machine learning, that allows us to achieve both data as well as model parallelism simultaneously. In addition to being parallelizable, our algorithm can also easily be made asynchronous. In order to demonstrate the effectiveness of our method, we solve a very large multi-class classification problem on the reddit dataset with data and parameter sizes of 200 GB and 300 GB respectively. Such a scale of data calls for simultaneous data and model parallelism which is where DS-MLR fits in. </small></p>
    <!-- 25 out of 215 (11.6%) -->
  </li>
  
  <li class="paper">
    <span class="title">
      Extreme Stochastic Variational Inference: Distributed and Asynchronous <br>
    </span>
    Jiong Zhang*, <span class="me">Parameswaran Raman*</span>, Shihao Ji, Hsiang-Fu Yu, S.V.N Vishwanathan (* equally contributed)<br>
    <b>pre-print</b>
    <a href="https://arxiv.org/pdf/1605.09499v3.pdf">[arXiv]</a>
    <!--a href="robirank.html">[Code]</a--><br/>
    <p><small>Summary: We propose extreme stochastic variational inference (ESVI), an
  asynchronous and lock-free algorithm to perform variational inference
  on massive real world datasets. Stochastic variational inference
  (SVI), the state-of-the-art algorithm for scaling variational
  inference to large-datasets, is inherently serial. Moreover, it
  requires the parameters to fit in the memory of a single processor;
  this is problematic when the number of parameters is in billions. ESVI
  overcomes these limitations by requiring that each processor only
  access a subset of the data and a subset of the parameters, thus
  providing data and model parallelism simultaneously.  We demonstrate
  the effectiveness of ESVI by running Latent Dirichlet Allocation (LDA)
  on UMBC-3B, a dataset that has a 3 million words in the vocabulary and
  3 billion tokens. To best of our knowledge, this is an order of
  magnitude larger than the largest dataset on which results using
  variational inference have been reported in literature.  In our
  experiments, we found that ESVI outperforms VI and SVI, and also
  achieves a better quality solution.  In addition, we propose a
  strategy to speed up computation and save memory when fitting large
  number of topics. </small></p>
  <!-- 25 out of 215 (11.6%) -->
  </li>

</ul>

<!--hr-->
<h2> Course Projects </h2>
<ul>
  <li class="paper">
    <span class="title">
      Optimization on the surface of the (Hyper)-Sphere <br>
    </span>
    <span class="me">Parameswaran Raman</span>, Jiasen Yang <br>
    Course Project, CS 520: Computational Methods in Optimization (Purdue University) <br>
    <a href="static/pub/cs520report.pdf">[pdf]</a>
  </li>

  <li class="paper">
    <span class="title">
      Target Score Prediction in the game of Cricket <br>
    </span>
    Sethuraman K, <span class="me">Parameswaran Raman</span>, Vijay Ramakrishnan <br>
    Course Project, CS 7641: Machine Learning (Georgia Institute of Technology) <br>
    <a href="static/pub/ML_Project_CS7641_report.pdf">[pdf]</a>
  </li>
  
  <li class="paper">
    <span class="title">
      PiX-C, Pictures: Express & Communicate (Augmenting Communication with Visual Input for Children in the Autism Spectrum) <br>
    </span>
    Narayanan Ramakrishnan, <span class="me">Parameswaran Raman</span>, Manohar Ganesan, Gourab Kar, Dr Gregory D. Abowd (Georgia Institute of Technology) <br>
    Poster at UIST 2010 <br>
    Course Project, CS 7650: Natural Language (with Ramakrishnan CH, Suman Manjunath) <br>
    <a href="static/pub/NLP_TermProject.pdf">[slides]</a>
    <a href="static/pub/PiX-C_Poster.pdf">[poster]</a>
    <a href="http://www.naduism.com/projects/#adaptivekb">[video]</a>
  </li>
</ul>

<h2> Other Papers </h2>
<p>Worked on during my Masters and Undergrad. During my Masters, I worked with Prof. <a href="http://sonify.psych.gatech.edu/~walkerb/">Bruce Walker</a> in the Sonification Lab, Georgia Tech on Auditory Interfaces/Human Computer Interaction.</p>
<ul>
  <li class="paper">
    <span class="title">
      Reducing repetitive development tasks in auditory menu displays with the auditory menu library <br>
    </span>
    <span class="me">Parameswaran Raman</span>, Benjamin Davison, Myounghoon "Philart" Jeon, Bruce N. Walker <br>
    <b>Proceedings of the 16th International Conference on Auditory Display (ICAD) 2010 </b><br>
    <a href="static/pub/AML_ICAD_2010.pdf">[pdf]</a>
  </li>
  
  <li class="paper">
    <span class="title">
      Advanced Auditory Menus for Universal Access to Electronic Devices <br>
    </span>
    Myounghoon "Philart" Jeon, Benjamin Davison, Jeff Wilson, <span class="me">Parameswaran Raman</span>, Bruce N. Walker <br>
    <b>Proceedings of CSUN International Technology & Persons with Disabilities Conference 2010</b><br>
  </li>

  <li class="paper">
    <span class="title">
        ENGIN (Exploring Next Generation IN-vehicle INterfaces): Drawing a New Conceptual Framework through Iterative Participatory Processes <br> 
    </span>
    Myounghoon "Philart" Jeon, Jonathan Schuett, Jung-Bin Yim, <span class="me">Parameswaran Raman</span>, Bruce N. Walker <br>
    <b>Adjunct Proceedings of the 3rd International Conference on Automotive User Interfaces and Interactive Vehicular Applications 2010</b><br>
  </li>

  <li class="paper">
    <span class="title">
        PINE-guided cache replacement policy for location-dependent data in mobile environment <br>
    </span>
    Mary Magdalene Jane, <span class="me">Parameswaran Raman</span>, Nadarajan R, Maytham Safar <br>
    <b>Proceedings of the First international conference on Pervasive Technologies Related to Assistive Environments, PETRA 2008</b><br>
    <a href="static/pub/PINE.pdf">[pdf]</a>
  </li>

  <li class="paper">
    <span class="title">
        Weighted Angular Distance Based Cache Replacement Strategy for Location-Dependent Data in Wireless Environment <br>
    </span>
    <span class="me">Parameswaran Raman</span>, Raghavendra Prasad, Nadarajan R, Mary Magdalene Jane <br>
    <b>Proceedings of the DCCA Conference, JORDAN 2007</b><br>
  </li>

</ul>

<hr>
<h2> Posters </h2>
<ul>
  <li class="paper">
    <span class="title">
      Relevancy Prediction of Micro-blog Questions in an Educational Setting<br>
    </span>
    Mariheida C&oacute;rdova S&aacute;nchez, <span class="me">Parameswaran Raman</span>, Luo Si, Jason Fish <br>
    <b>Proceedings of the Seventh International Conference on Educational Data Mining (EDM) 2014 </b><br>
    <a href="static/pub/Poster_EDM_2014.pdf">[pdf]</a>
    <!-- 25 out of 215 (11.6%) -->
  </li>
</ul>
